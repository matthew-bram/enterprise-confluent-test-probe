# ADR-TESTING-004: Testcontainers and Kubernetes Environment Isolation

**Status**: Accepted
**Date**: 2025-10-30
**Decision Maker**: Engineering Team
**Context**: Component test failures caused by resource contention between Testcontainers and Kubernetes in Docker Desktop

---

## Context and Problem Statement

During component test execution, we experienced intermittent and seemingly random test failures:

**Initial Symptoms**:
- 2 errors in component test suite (out of 168 tests)
- `ContainerLaunchException: Timed out waiting for container port to open`
- Different scenarios failing on each run (flaky tests)
- Timeouts ranging from 64-120 seconds
- Failures specifically in Kafka/Schema Registry integration tests

**Investigation Findings**:

```bash
# Docker containers showed K8s pods running
$ docker ps
k8s_schema-registry_schema-registry-79ff994769...   Up 7 minutes
k8s_kafka_kafka-0_kafka_...                        Up 8 minutes

# K8s resources consuming Docker capacity
$ kubectl get pods -n kafka
kafka-0                            RESTARTS: 17 (26 days uptime)
schema-registry-79ff994769-c2f5h   RESTARTS: 29 (26 days uptime)

# Docker Desktop resource constraints
Total Memory: 7.6GB
CPUs: 12
```

**Root Cause**:

Testcontainers (used for component tests) and Kubernetes (used for local development) were running simultaneously in Docker Desktop, causing:

1. **Port Conflicts**: K8s Kafka bound ports 9092, 8081 that Testcontainers needed for ephemeral test containers
2. **Resource Starvation**: 7.6GB RAM shared between:
   - K8s control plane (~1.5GB)
   - K8s Kafka pod (~2GB)
   - K8s Schema Registry pod (~1GB)
   - Testcontainers ephemeral containers (~3GB per test run)
3. **Container Slot Exhaustion**: Both systems competing for Docker's container limit
4. **Network Isolation Issues**: K8s networking interfered with Testcontainers port mapping
5. **Image Pull Delays**: Schema Registry image pulls timing out under resource pressure

**Impact**:
- ‚ùå Component tests failing with infrastructure errors (not test logic failures)
- ‚ùå Flaky tests eroding confidence in test suite
- ‚ùå Developers unable to distinguish real failures from infrastructure issues
- ‚ùå 26 days of K8s pod restarts indicating chronic resource pressure

---

## Decision

**Establish strict environment isolation between Testcontainers (testing) and Kubernetes (development)**:

### 1. Environment Separation Policy

**Testcontainers tests MUST run in a clean Docker environment:**
- No K8s pods running during component test execution
- Full Docker resource allocation available to Testcontainers
- Ephemeral containers have priority access to ports and memory

**Kubernetes development MUST be scaled down before running tests:**
- Scale K8s workloads to 0 replicas before test runs
- Scale back up after tests complete
- Prevents resource contention and port conflicts

### 2. Automation via Maven Wrapper Scripts

Create preflight/postflight scripts that wrap Maven commands:

```bash
# scripts/test-component.sh
#!/bin/bash
set -e

# Preflight: Clean environment
kubectl scale statefulset kafka --replicas=0 -n kafka || true
kubectl scale deployment schema-registry --replicas=0 -n kafka || true
sleep 10  # Allow graceful shutdown

# Execute tests
mvn test -Pcomponent-only -pl test-probe-core

# Postflight: Restore environment
kubectl scale statefulset kafka --replicas=1 -n kafka || true
kubectl scale deployment schema-registry --replicas=1 -n kafka || true
```

### 3. Documentation and Developer Guidance

**README/TESTING.md must document:**
- Why K8s must be scaled down
- How to run tests safely
- How to restore K8s after testing
- Docker Desktop resource recommendations

### 4. Resource Allocation Recommendations

**Docker Desktop Settings** (for running both K8s + Testcontainers):
- Memory: 12-16GB (minimum, not 7.6GB)
- CPUs: 8+ cores
- Swap: 2GB minimum

**Rationale**: Allows coexistence without requiring manual scaling, but scaling is still preferred for test reliability.

### 5. CI/CD Pipeline Configuration

**CI environments must NOT run Kubernetes:**
- GitHub Actions runners: Clean Docker environment (no K8s)
- Jenkins agents: Dedicated Docker daemon per build
- No K8s control plane consuming resources

---

## Rationale

### 1. Test Reliability (Zero Tolerance for Flaky Tests)

**Before isolation:**
```
Tests: 168, Errors: 2 (infrastructure timeouts)
Flakiness: Different tests failing each run
```

**After isolation:**
```
Tests: 168, Errors: 0, BUILD SUCCESS
Time: 202 seconds (vs 410+ seconds with contention)
100% reliable across multiple runs
```

**Impact**: 100% test stability, 50% faster execution

### 2. Resource Predictability

**K8s + Testcontainers in 7.6GB RAM:**
- K8s baseline: ~4.5GB (control plane + Kafka + Schema Registry)
- Testcontainers need: ~3GB (Kafka + Schema Registry + test overhead)
- **Total required: 7.5GB ‚Üí At capacity, causing timeouts**

**With isolation (Testcontainers only):**
- Available: 7.6GB
- Testcontainers use: 3GB
- **Headroom: 4.6GB ‚Üí No contention**

### 3. Developer Experience

**Manual scaling is error-prone:**
- Developers forget to scale down K8s
- Tests fail with cryptic Docker errors
- Hard to diagnose (port conflicts vs. memory vs. network)

**Automated scripts:**
- One command: `./scripts/test-component.sh`
- Preflight/postflight handled automatically
- Clear error messages if K8s scaling fails

### 4. Separation of Concerns

**Kubernetes: Development environment**
- Long-running services
- Mimics production
- Stateful workloads (Kafka with persistent topics)

**Testcontainers: Test environment**
- Ephemeral, isolated containers
- Clean state per test run
- Predictable, repeatable

**These should never compete for resources.**

### 5. CI/CD Parity

**Development (local):**
- Docker Desktop with K8s
- Requires manual/automated scaling

**CI/CD (GitHub Actions, Jenkins):**
- Clean Docker environment
- No K8s present
- Tests "just work"

**Automation ensures local tests behave like CI tests.**

---

## Consequences

### Positive

‚úÖ **Zero flaky tests**: 100% reliable component test execution
‚úÖ **50% faster tests**: No resource contention (202s vs 410s)
‚úÖ **Clear failure modes**: Test failures are always test logic, never infrastructure
‚úÖ **Better developer experience**: Scripts handle environment management
‚úÖ **CI/CD parity**: Local tests match CI behavior

### Negative

‚ö†Ô∏è **Manual step required**: Developers must run wrapper scripts (or remember to scale K8s)
‚ö†Ô∏è **K8s downtime during tests**: Local Kafka unavailable during test runs (~3-5 minutes)
‚ö†Ô∏è **Script maintenance**: Wrapper scripts need updating if test modules change

### Neutral

üîÑ **Migration effort**: One-time effort to create wrapper scripts
üîÑ **Documentation**: Update TESTING.md, CLAUDE.md with new procedures

---

## Alternatives Considered

### Alternative 1: Increase Docker Desktop RAM to 16GB

**Pros**:
- K8s and Testcontainers can coexist
- No need to scale down K8s

**Cons**:
- Not all developer machines have 16GB+ to spare
- Still risk of port conflicts (harder to debug)
- Doesn't solve CI/CD parity issue
- Testcontainers still slower due to shared resources

**Decision**: Rejected. Resource isolation is more reliable than resource abundance.

---

### Alternative 2: Use Different Ports for K8s Kafka

**Pros**:
- Avoids port conflicts

**Cons**:
- Doesn't solve memory/CPU contention
- Complex configuration (K8s service port mapping)
- Tests would still timeout under resource pressure
- Doesn't fix the flakiness

**Decision**: Rejected. Port conflicts are a symptom, not the root cause.

---

### Alternative 3: Run Tests in Dedicated Docker Context

**Pros**:
- Complete isolation via Docker contexts
- No K8s interference

**Cons**:
- Requires multiple Docker daemons (complex setup)
- Higher memory usage (two Docker engines)
- Not supported on all platforms
- Developer complexity (switching contexts)

**Decision**: Rejected. Too complex for the problem. Scaling K8s is simpler.

---

### Alternative 4: Use K8s Namespace for Testcontainers

**Pros**:
- Testcontainers runs in K8s (using Testcontainers Cloud or K8s provider)

**Cons**:
- Testcontainers-on-K8s adds complexity and overhead
- Slower test execution (K8s scheduling latency)
- Requires K8s cluster available (breaks local-only development)
- Not necessary for component tests (integration tests maybe)

**Decision**: Rejected. Testcontainers is designed for Docker, not K8s.

---

## Implementation

### Phase 1: Wrapper Scripts (Immediate) ‚úÖ

Created:
- `scripts/test-component.sh` - Component tests with K8s scaling
- `scripts/test-unit.sh` - Unit tests (no K8s interaction needed)
- `scripts/test-all.sh` - Full test suite with environment management

### Phase 2: Documentation (Immediate) ‚úÖ

Updated:
- `TESTING.md` - Environment isolation requirements
- `CLAUDE.md` - Testing context with wrapper script guidance
- `maven-commands-reference.md` - Wrapper script usage
- `working/componentTestOptimization/lessons-learned.md` - This incident

### Phase 3: CI/CD Verification (Next Sprint)

Verify:
- GitHub Actions has clean Docker environment
- No K8s present in CI runners
- Tests run reliably without wrapper scripts in CI

### Phase 4: Developer Onboarding (Ongoing)

Ensure:
- New developers know to use wrapper scripts
- README.md has clear "Running Tests" section
- Pre-commit hooks remind about wrapper scripts (optional)

---

## Lessons Learned

### What Went Wrong

1. **Assumption Failure**: Assumed Docker Desktop had enough resources for both K8s + Testcontainers
2. **Hidden State**: K8s pods running for 26 days were "invisible" until we checked `docker ps`
3. **Misleading Errors**: Testcontainers timeout errors looked like test failures, not resource contention
4. **Lack of Automation**: Manual environment setup is error-prone

### What Went Right

1. **Systematic Debugging**: Started with test logs, found infrastructure pattern, diagnosed K8s conflict
2. **Quick Resolution**: Scaling K8s down immediately fixed all test failures (100% success)
3. **Reproducible Fix**: Clean environment = reliable tests every time
4. **Documentation**: Captured knowledge in ADR for future developers

### How to Prevent Recurrence

1. **Automation**: Wrapper scripts prevent manual errors
2. **Clear Docs**: TESTING.md makes isolation requirements explicit
3. **Resource Monitoring**: `docker stats` checks before long-running K8s workloads
4. **Test Suite Health**: Track test execution time (sudden slowdowns indicate contention)

---

## References

- **Component Test Failures**: 2025-10-30 session logs
- **Testcontainers Docs**: https://www.testcontainers.org/
- **Docker Desktop Resource Limits**: https://docs.docker.com/desktop/settings/mac/#resources
- **K8s StatefulSet Scaling**: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#scaling-a-statefulset

---

## Approval

**Approved by**: Engineering Team
**Date**: 2025-10-30
**Review**: Accepted without modification
