# Block Storage Service Architecture

**Version:** 1.0
**Date:** 2025-10-27
**Module:** test-probe-services
**Status:** Architecture Complete, Implementation Pending

---

## Executive Summary

The Block Storage Service provides a unified abstraction for fetching test artifacts from cloud storage providers (AWS S3, Azure Blob Storage, GCP Cloud Storage) and local in-memory filesystem (JIMFS). It streams artifacts to JIMFS for test execution, validates structure, parses configurations, and uploads evidence back to storage after execution.

**Key Capabilities:**
- Multi-cloud provider support (AWS, Azure, GCP, Local)
- SDK-native streaming for efficiency
- Shared JIMFS instance with per-test isolation
- Automatic validation and parsing
- Clean lifecycle management

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    ServiceDSL Builder                             │
│  Orchestrates 3-phase lifecycle (preFlight/initialize/finalCheck)│
└──────────────────┬──────────────────────────────────────────────┘
                   │
                   ▼
┌─────────────────────────────────────────────────────────────────┐
│           ProbeStorageService (Builder Module)                    │
│  Provider-specific implementations with 3-phase lifecycle         │
└──────┬──────────┬──────────┬──────────┬──────────────────────────┘
       │          │          │          │
       ▼          ▼          ▼          ▼
┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐
│   Local    │  │    AWS     │  │   Azure    │  │    GCP     │
│  Builder   │  │  Builder   │  │  Builder   │  │  Builder   │
│  Module    │  │  Module    │  │  Module    │  │  Module    │
└────────────┘  └────────────┘  └────────────┘  └────────────┘
       │                │                │                │
       │ Uses           │ Uses           │ Uses           │ Uses
       ▼                ▼                ▼                ▼
┌─────────────────────────────────────────────────────────────────┐
│                   Shared Components                               │
│  - JimfsManager (shared FileSystem singleton)                    │
│  - TopicDirectiveMapper (YAML parser)                            │
│  - BlockStorageConfig (config loader)                            │
│  - BlockStorageExceptions (error types)                          │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│               JIMFS (In-Memory FileSystem)                        │
│  /testId-1/features/, /testId-1/evidence/                        │
│  /testId-2/features/, /testId-2/evidence/                        │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│              StorageServiceFunctions (Curried)                    │
│  Extracted functions passed down actor hierarchy                 │
└─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                    BlockStorageActor                              │
│  Coordinates fetch/upload operations via curried functions       │
└─────────────────────────────────────────────────────────────────┘
```

---

## Component Breakdown

### 1. ProbeStorageService Builder Modules

**Purpose:** Provider-specific implementations with 3-phase lifecycle

**Trait Definition:**
```scala
trait ProbeStorageService extends Feature with BuilderModule {
  def fetchFromBlockStorage(testId: UUID, bucket: String)(implicit ec: ExecutionContext): Future[BlockStorageDirective]
  def loadToBlockStorage(testId: UUID, bucket: String, evidence: String)(implicit ec: ExecutionContext): Future[Unit]
}
```

**Builder Module Implementations:**
- **LocalBlockStorageService** - JIMFS only, for testing/development
- **AwsBlockStorageService** - AWS S3 integration with SDK
- **AzureBlockStorageService** - Azure Blob Storage integration
- **GcpBlockStorageService** - GCP Cloud Storage integration

**3-Phase Lifecycle:**

**Phase 1: preFlight()**
- Validates config exists
- Checks provider matches module ("local", "aws", "azure", "gcp")
- Validates required config fields (region, credentials, etc.)
- Returns **same** context (no decoration)

**Phase 2: initialize()**
- Creates SDK clients (S3Client, BlobServiceClient, Storage)
- Initializes JimfsManager
- Initializes TopicDirectiveMapper
- Loads BlockStorageConfig
- **Decorates** context via `ctx.withStorageService(this)`
- Returns decorated context

**Phase 3: finalCheck()**
- Validates storageService is in context
- Validates all @volatile vars initialized
- Returns **same** context

**Example Structure:**
```scala
class LocalBlockStorageService extends ProbeStorageService {
  @volatile private var jimfsManager: JimfsManager = _
  @volatile private var topicDirectiveMapper: TopicDirectiveMapper = _
  @volatile private var config: BlockStorageConfig = _

  override def preFlight(ctx: BuilderContext)(implicit ec: ExecutionContext): Future[BuilderContext] = Future {
    // Validate config
    ctx
  }

  override def initialize(ctx: BuilderContext)(implicit ec: ExecutionContext): Future[BuilderContext] = Future {
    // Create components, decorate context
    ctx.withStorageService(this)
  }

  override def finalCheck(ctx: BuilderContext)(implicit ec: ExecutionContext): Future[BuilderContext] = Future {
    // Validate initialization
    ctx
  }

  override def fetchFromBlockStorage(...): Future[BlockStorageDirective] = Future {
    // Business logic using initialized components
  }
}
```

---

### 2. Shared Components

These components are used by all builder modules:

#### JimfsManager

**Purpose:** Singleton FileSystem manager with directory lifecycle

**API:**
```scala
class JimfsManager {
  def createTestDirectory(testId: UUID): Path
  def getTestDirectory(testId: UUID): Path
  def deleteTestDirectory(testId: UUID): Unit
  def testDirectoryExists(testId: UUID): Boolean
  def listTestDirectories(): List[UUID]
  def cleanupAll(): Unit
}
```

**Usage:** Initialized once per module, reused across all operations

---

#### TopicDirectiveMapper

**Purpose:** Parse YAML/JSON into List[TopicDirective]

**API:**
```scala
class TopicDirectiveMapper {
  def parse(yamlContent: String): List[TopicDirective]
}
```

**Parser:** Circe YAML with custom decoders for event filters

**Usage:** Initialized once per module, reused for all topic directive parsing

---

#### BlockStorageConfig

**Purpose:** Provider-specific configuration

**Structure:**
```scala
case class BlockStorageConfig(
  provider: String,
  topicDirectiveFileName: String,
  local: LocalBlockStorageConfig,
  aws: AwsBlockStorageConfig,
  azure: AzureBlockStorageConfig,
  gcp: GcpBlockStorageConfig
)
```

**Loading:** `BlockStorageConfig.fromConfig(ctx.config.get)` in preFlight phase

---

#### BlockStorageExceptions

**Custom exception hierarchy:**
- BlockStorageException (base)
- MissingFeaturesDirectoryException
- EmptyFeaturesDirectoryException
- MissingTopicDirectiveFileException
- InvalidTopicDirectiveFormatException
- BucketUriParseException
- StreamingException

**Usage:** Thrown by modules during validation and business logic

---

### 3. StorageServiceFunctions (Curried Functions)

**Purpose:** Decouple actors from service module implementations

**Pattern:** Same as VaultServiceFunctions - pure function bundle extracted from ProbeStorageService

**Definition:**
```scala
case class StorageServiceFunctions(
  fetchFromBlockStorage: (UUID, String) => ExecutionContext ?=> Future[BlockStorageDirective],
  loadToBlockStorage: (UUID, String, String) => ExecutionContext ?=> Future[Unit]
)
```

**Extraction:**
```scala
object StorageServiceFunctions {
  def fromService(service: ProbeStorageService): StorageServiceFunctions =
    StorageServiceFunctions(
      fetchFromBlockStorage = service.fetchFromBlockStorage,
      loadToBlockStorage = service.loadToBlockStorage
    )
}
```

**Flow:**
1. DefaultActorSystem calls `fromService()` on initialized module
2. Functions bundled into ServiceFunctionsContext
3. Context passed down actor hierarchy: GuardianActor → QueueActor → TestExecutionActor → BlockStorageActor
4. BlockStorageActor receives curried functions in constructor

---

## Data Flow: Builder Module Initialization

```
ServiceDSL.build() {

  // Phase 1: preFlight (All Modules)
  for {
    ctx0 <- configModule.preFlight(emptyContext)
    ctx0 <- actorSystemModule.preFlight(ctx0)
    ctx0 <- storageModule.preFlight(ctx0)         // Validates config.provider matches
    ctx0 <- vaultModule.preFlight(ctx0)

    // Phase 2: initialize (Context Threading)
    ctx1 <- configModule.initialize(ctx0)         // Adds Config
    ctx2 <- actorSystemModule.initialize(ctx1)    // Adds ActorSystem
    ctx3 <- storageModule.initialize(ctx2)        // Adds StorageService, creates JimfsManager/mapper
    ctx4 <- vaultModule.initialize(ctx3)          // Adds VaultService

    // Phase 3: finalCheck (All Modules)
    ctx4 <- configModule.finalCheck(ctx4)
    ctx4 <- actorSystemModule.finalCheck(ctx4)
    ctx4 <- storageModule.finalCheck(ctx4)        // Validates all @volatile vars initialized
    ctx4 <- vaultModule.finalCheck(ctx4)
  } yield ctx4.toServiceContext
}
```

**storageModule.initialize() Details:**
```scala
override def initialize(ctx: BuilderContext)(implicit ec: ExecutionContext): Future[BuilderContext] = Future {
  config = BlockStorageConfig.fromConfig(ctx.config.get)
  jimfsManager = new JimfsManager()
  topicDirectiveMapper = new TopicDirectiveMapper()

  // Provider-specific: Create SDK clients
  s3Client = S3Client.builder().region(config.aws.region).build()  // AWS example

  ctx.withStorageService(this)  // Decorate context
}
```

---

## Data Flow: Fetch from Block Storage

```
1. Curried Function Call (via BlockStorageActor)
   fetchFromBlockStorage(testId, bucketUri)(using ec)

2. Module Business Logic (AwsBlockStorageService example)
   - Parse S3 URI: s3://bucket/path → (bucket, prefix)
   - Create test directory: jimfsManager.createTestDirectory(testId) → /testId/

3. SDK Streaming (Download)
   s3Client.listObjectsV2(bucket, prefix)
   for each object:
     s3Client.getObject(request, /testId/relative/path)

4. Validation
   - Check /testId/features/ exists and not empty
   - Find /testId/test-config.yaml

5. Parsing
   topicDirectiveMapper.parse(yamlContent) → List[TopicDirective]

6. Evidence Directory
   Files.createDirectories(/testId/evidence/)

7. Construct Directive
   BlockStorageDirective(
     jimfsLocation = "/testId/features",
     evidenceDir = "/testId/evidence",
     topicDirectives = parsedDirectives,
     bucket = bucketUri
   )

8. Return
   Future.successful(directive)
```

---

## Data Flow: Upload to Block Storage

```
1. Curried Function Call (via BlockStorageActor)
   loadToBlockStorage(testId, bucketUri, evidenceDir)(using ec)

2. Module Business Logic (AwsBlockStorageService example)
   - Parse S3 URI: s3://bucket/path → (bucket, prefix)

3. SDK Streaming (Upload)
   Files.walk(Path.of(evidenceDir))
   for each file:
     s3Client.putObject(request, file) → s3://bucket/evidence/file

4. JIMFS Cleanup
   jimfsManager.deleteTestDirectory(testId)
   Recursively delete /testId/ and all contents

5. Return
   Future.successful(())
```

---

## Provider Implementations

### Local Block Storage Service

**Purpose:** Testing and development (no cloud storage)

**Fetch Behavior:**
- Assumes features already exist in JIMFS
- Parses topic directive file
- Creates evidence directory
- Returns directive (no actual download)

**Upload Behavior:**
- No-op (evidence remains in JIMFS for inspection)
- Or optionally copies to /tmp for debugging

**Use Cases:**
- Unit tests (fast, no external dependencies)
- Local development
- CI/CD without cloud credentials

---

### AWS Block Storage Service

**SDK:** AWS SDK for Java v2 (software.amazon.awssdk:s3)

**Fetch Implementation:**
1. Parse S3 URI: `s3://bucket-name/path/to/test/` → (bucket, prefix)
2. List objects: `S3Client.listObjectsV2()`
3. Stream each object: `S3Client.getObject(request, targetPath)`
4. Validate and parse

**Upload Implementation:**
1. Walk evidence directory: `Files.walk(evidencePath)`
2. Stream each file: `S3Client.putObject(request, filePath)`
3. Cleanup JIMFS

**Configuration:**
- Region (required)
- Timeout
- Retry attempts

---

### Azure Block Storage Service

**SDK:** Azure Storage Blob SDK (com.azure:azure-storage-blob)

**Fetch Implementation:**
1. Parse Azure URI: `https://account.blob.core.windows.net/container/path/`
2. List blobs: `BlobContainerClient.listBlobs()`
3. Stream each blob: `BlobClient.downloadStream(outputStream)`
4. Validate and parse

**Upload Implementation:**
1. Walk evidence directory
2. Stream each file: `BlobClient.uploadFromFile()`
3. Cleanup JIMFS

**Configuration:**
- Storage account name (required)
- Storage account key (required)
- Timeout
- Retry attempts

---

### GCP Block Storage Service

**SDK:** GCP Cloud Storage (com.google.cloud:google-cloud-storage)

**Fetch Implementation:**
1. Parse GCS URI: `gs://bucket-name/path/to/test/`
2. List objects: `Bucket.list(prefix)`
3. Stream each object: `Blob.downloadTo(outputStream)`
4. Validate and parse

**Upload Implementation:**
1. Walk evidence directory
2. Stream each file: `Storage.create(blobInfo, inputStream)`
3. Cleanup JIMFS

**Configuration:**
- Project ID (required)
- Service account key (optional, uses default credentials if not provided)
- Timeout
- Retry attempts

---

## Error Handling Strategy

### Custom Exceptions

**Hierarchy:**
```
BlockStorageException (base)
├── MissingFeaturesDirectoryException
├── EmptyFeaturesDirectoryException
├── MissingTopicDirectiveFileException
├── InvalidTopicDirectiveFormatException
├── BucketUriParseException
└── StreamingException
```

**Location:** `test-probe-common/src/main/scala/io/distia/probe/common/exceptions/BlockStorageExceptions.scala`

---

### Error Handling Patterns

**Parse Errors:**
```scala
Try {
  topicDirectiveMapper.parse(yamlContent)
} match {
  case Success(directives) => directives
  case Failure(ex) =>
    throw InvalidTopicDirectiveFormatException(
      s"Failed to parse topic directive file: ${ex.getMessage}",
      ex
    )
}
```

**SDK Errors:**
```scala
try {
  s3Client.getObject(request, targetPath)
} catch {
  case ex: S3Exception =>
    jimfsManager.deleteTestDirectory(testId)
    throw BlockStorageException(
      s"Failed to fetch from S3: ${ex.getMessage}",
      ex
    )
}
```

---

## Configuration Management

### reference.conf Structure

```hocon
test-probe {
  services {
    block-storage {
      provider = "local"
      provider = ${?BLOCK_STORAGE_PROVIDER}

      topic-directive-file-name = "test-config.yaml"

      local {}

      aws {
        region = "us-east-1"
        region = ${?AWS_REGION}
        timeout = 30s
        retry-attempts = 3
      }

      azure {
        storage-account-name = ${?AZURE_STORAGE_ACCOUNT_NAME}
        storage-account-key = ${?AZURE_STORAGE_ACCOUNT_KEY}
        timeout = 30s
        retry-attempts = 3
      }

      gcp {
        project-id = ${?GCP_PROJECT_ID}
        service-account-key = ${?GCP_SERVICE_ACCOUNT_KEY}
        timeout = 30s
        retry-attempts = 3
      }
    }
  }
}
```

---

## Integration with Actor System

### Curried Functions Pattern

**BlockStorageActor** receives curried functions (NOT the service module directly):

```scala
object BlockStorageActor {
  def apply(
    fetchFromBlockStorage: (UUID, String) => ExecutionContext ?=> Future[BlockStorageDirective],
    loadToBlockStorage: (UUID, String, String) => ExecutionContext ?=> Future[Unit]
  ): Behavior[BlockStorageCommand] = Behaviors.setup { ctx =>
    given ec: ExecutionContext = ctx.executionContext

    Behaviors.receiveMessage {
      case FetchFromBlockStorage(testId, bucketUri) =>
        ctx.pipeToSelf(fetchFromBlockStorage(testId, bucketUri)) {
          case Success(directive) => FetchSuccess(directive)
          case Failure(ex) => FetchFailure(ex)
        }
        Behaviors.same

      case UploadToBlockStorage(testId, bucketUri, evidenceDir) =>
        ctx.pipeToSelf(loadToBlockStorage(testId, bucketUri, evidenceDir)) {
          case Success(_) => UploadSuccess()
          case Failure(ex) => UploadFailure(ex)
        }
        Behaviors.same
    }
  }
}
```

**Function Extraction in DefaultActorSystem:**
```scala
override def initialize(ctx: BuilderContext)(implicit ec: ExecutionContext): Future[BuilderContext] = Future {
  // After storage module initialized
  val storageFunctions = StorageServiceFunctions.fromService(ctx.storageService.get)

  // Bundle into ServiceFunctionsContext
  val functionContext = ServiceFunctionsContext(
    vaultFunctions = vaultFunctions,
    storageFunctions = storageFunctions
  )

  // Pass down actor hierarchy
  val guardianActor = ctx.actorSystem.get.systemActorOf(
    GuardianActor(functionContext),
    "guardian"
  )

  ctx.withGuardianActor(guardianActor)
}
```

**Benefits:**
- Actors decoupled from module implementations
- Pure functions testable independently
- Module lifecycle managed by ServiceDSL, not actors
- Consistent pattern with VaultServiceFunctions

---

## Testing Strategy (Future Phase)

### Unit Tests

**JimfsManager:**
- Test directory creation/deletion
- Test concurrent directory operations
- Test cleanup all

**TopicDirectiveMapper:**
- Test valid YAML parsing
- Test invalid YAML error messages
- Test all field combinations

**LocalBlockStorageService:**
- Test fetch validation
- Test evidence directory creation
- Test error cases (missing features, invalid YAML)

---

### Component Tests

**Integration with JIMFS:**
- Test streaming large files
- Test concurrent test directories
- Test cleanup after errors

**Provider-Specific:**
- Mock S3/Azure/GCP clients
- Test streaming operations
- Test retry logic
- Test error scenarios

---

## Performance Considerations

### Memory Usage

**Per-Test Footprint:**
- JIMFS metadata: ~1 KB
- Feature files: Varies (typically 10-100 KB total)
- Evidence files: Varies (typically 100 KB - 1 MB)

**Concurrent Tests:**
- 10 tests ≈ 6 MB
- 100 tests ≈ 60 MB

**Acceptable:** Modern JVMs handle this easily

---

### Streaming Performance

**SDK-Native Streaming:**
- AWS SDK: Optimized for S3 (8KB chunks)
- Azure SDK: Optimized for Blob Storage
- GCP SDK: Optimized for Cloud Storage

**No Additional Overhead:** Direct SDK usage avoids Pekko Streams ceremony

---

## Security Considerations

### Credentials Management

**AWS:** IAM roles or environment variables
**Azure:** Connection string or SAS token in environment variables
**GCP:** Service account key file or default credentials

**Best Practice:**
- Never hardcode credentials in config files
- Use environment variables: `${?AZURE_STORAGE_ACCOUNT_KEY}`
- Rotate credentials regularly

---

### Bucket Access Control

**Least Privilege:**
- Read-only access for fetch operations
- Write-only access for evidence upload (separate credentials if possible)
- Limit bucket access to specific prefixes

---

## Monitoring and Observability (Future)

### Metrics to Track

- Number of active test directories
- Total JIMFS memory usage
- Fetch operation latency
- Upload operation latency
- Error rates by provider
- Concurrent test count

### Logging

**Fetch Operation:**
```
INFO: Fetching from block storage [testId=${testId}, bucket=${bucketUri}]
INFO: Fetched 15 files from S3 [testId=${testId}, duration=2.3s]
INFO: Parsed 3 topic directives [testId=${testId}]
```

**Upload Operation:**
```
INFO: Uploading evidence to block storage [testId=${testId}, fileCount=5]
INFO: Uploaded evidence successfully [testId=${testId}, duration=1.8s]
INFO: Cleaned up JIMFS directory [testId=${testId}]
```

---

## References

- **ADR-STORAGE-001:** Block Storage Abstraction
- **ADR-STORAGE-002:** SDK Streaming Approach
- **ADR-STORAGE-003:** JIMFS Architecture
- **ADR-STORAGE-004:** Topic Directive Format
- **Implementation Plan:** BlockStorageImplementationPlan.md
- **Topic Directive Model:** topic-directive-model.md

---

**Last Updated:** 2025-10-27
**Author:** AI Engineering Team (Paired Programming Session)
**Status:** Ready for Implementation ✅
