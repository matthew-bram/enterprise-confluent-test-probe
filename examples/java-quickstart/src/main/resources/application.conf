# =============================================================================
# Test-Probe Java Quickstart - Local Development Configuration
# =============================================================================
# This configuration is designed for local development using Docker Compose.
# For production deployments, override settings via environment variables or
# by creating environment-specific configuration files (application-prod.conf).
#
# Configuration Loading Order (HOCON):
#   1. reference.conf (from test-probe-core, test-probe-services JARs)
#   2. application.conf (this file)
#   3. Environment variables (via ${?VAR_NAME} substitution)
#   4. System properties (-Dkey=value)
#
# HOCON Syntax Notes:
#   - ${VAR_NAME}  : Required substitution (fails if not found)
#   - ${?VAR_NAME} : Optional substitution (uses default if not found)
#   - Comments start with # or //
#   - Values can be strings, numbers, booleans, durations (10s, 5m), or lists
# =============================================================================

test-probe {

  # ===========================================================================
  # Core Module Configuration
  # ===========================================================================
  # The core module provides the actor system, test execution FSM, and
  # coordination logic. These settings control actor behavior, timeouts,
  # and supervision strategies.

  core {

    # -------------------------------------------------------------------------
    # Actor System Configuration
    # -------------------------------------------------------------------------
    # The actor system is the runtime for all Test-Probe actors (Guardian,
    # TestExecution, Queue, BlockStorage, Vault, Kafka Producer/Consumer).

    actor-system {
      # Actor system name (visible in logs and metrics)
      # Used to identify this Test-Probe instance in distributed systems
      # REQUIRED: ServiceDsl.build() validates this is present
      name = "quickstart"

      # Timeout for actor ask patterns (actor-to-actor request/response)
      # This is the maximum time to wait for a response from child actors
      # Default: 25s (must be < HTTP timeout to prevent client timeouts)
      #
      # Examples of ask pattern usage:
      #   - GuardianActor asking TestExecutionActor to start a test
      #   - TestExecutionActor asking BlockStorageActor to upload evidence
      #   - TestExecutionActor asking VaultActor to fetch credentials
      timeout = 25s

      # Shutdown timeout (graceful termination)
      # Maximum time to wait for all actors to stop during shutdown
      shutdown-timeout = 10s
    }

    # -------------------------------------------------------------------------
    # Queue Configuration
    # -------------------------------------------------------------------------
    # The QueueActor manages test execution concurrency with a FIFO queue.

    queue {
      # Maximum number of tests that can execute concurrently
      # Each test spawns a TestExecutionActor with 5 child actors
      # Recommendation: 5-10 for local dev, 20-50 for production
      max-concurrent-tests = 10

      # Maximum queue depth (pending tests)
      # Tests beyond this limit are rejected with back-pressure
      # Recommendation: 50-100 for local dev, 500-1000 for production
      max-queue-depth = 100
    }

    # -------------------------------------------------------------------------
    # Kafka Connection Settings
    # -------------------------------------------------------------------------
    # These settings connect Test-Probe to Kafka and Schema Registry.
    # In local development, these point to Docker Compose services.
    # In production, these are overridden via environment variables.

    kafka {
      # Kafka bootstrap servers (comma-separated list of broker addresses)
      # Local: Points to Docker Compose Kafka service (kafka:9092)
      # Production: Override with KAFKA_BOOTSTRAP_SERVERS env var
      #
      # Format: "host1:port1,host2:port2,host3:port3"
      # Example: "broker1.company.com:9092,broker2.company.com:9092"
      bootstrap-servers = "localhost:9092"
      bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}

      # Schema Registry URL (used for Avro and Protobuf serialization)
      # Local: Points to Docker Compose Schema Registry service
      # Production: Override with SCHEMA_REGISTRY_URL env var
      #
      # Schema Registry stores and versions Avro/Protobuf schemas
      # Producers register schemas, consumers fetch schemas by ID
      schema-registry-url = "http://localhost:8081"
      schema-registry-url = ${?SCHEMA_REGISTRY_URL}
    }

    # -------------------------------------------------------------------------
    # DSL (Domain-Specific Language) Configuration
    # -------------------------------------------------------------------------
    # The ProbeScalaDsl and ProbeJavaDsl provide simple APIs for producing
    # and consuming Kafka messages from Cucumber step definitions.
    #
    # Example usage in step definitions:
    #   ProbeJavaDsl.produceEvent(topic, event, headers);
    #   Optional<CloudEvent> event = ProbeJavaDsl.fetchConsumedEvent(topic);

    dsl {
      # Timeout for DSL operations (produceEvent, fetchConsumedEvent)
      # This is how long to wait for the Kafka producer/consumer actors
      # to complete the operation before timing out
      #
      # Recommendation: Keep short (3s) to fail fast in tests
      # If tests timeout frequently, increase to 5s or 10s
      ask-timeout = 3s
    }
  }

  # ===========================================================================
  # Interfaces Module Configuration
  # ===========================================================================
  # The interfaces module provides API endpoints for test submission and management.

  interfaces {

    # -------------------------------------------------------------------------
    # REST API Configuration
    # -------------------------------------------------------------------------
    # The REST interface exposes HTTP endpoints for test operations.

    rest {
      # Host to bind HTTP server
      # Use "localhost" for local dev, "0.0.0.0" for container/K8s
      host = "localhost"
      host = ${?TEST_PROBE_REST_HOST}

      # Port to bind HTTP server
      # Default: 8080 (standard HTTP alternative port)
      port = 8080
      port = ${?TEST_PROBE_REST_PORT}

      # Timeout for HTTP requests
      # Maximum time to wait for REST operations to complete
      # Recommendation: 30s (should be > actor timeout)
      timeout = 30s

      # CORS configuration (for web dashboard integrations)
      cors {
        enabled = true
        allowed-origins = ["*"]  # Production: specify exact origins
      }
    }
  }

  # ===========================================================================
  # Services Module Configuration
  # ===========================================================================
  # The services module provides integrations with external systems:
  #   - Vault: Fetches OAuth credentials for Kafka authentication
  #   - Block Storage: Stores feature files and test evidence

  services {

    # -------------------------------------------------------------------------
    # Vault Configuration
    # -------------------------------------------------------------------------
    # Vault provides OAuth credentials (client ID, client secret) for Kafka
    # authentication. In production, this integrates with CSP vault functions
    # (AWS Lambda, Azure Functions, GCP Cloud Functions).
    #
    # For local development, we use a "local" provider (no-op vault) that
    # returns hardcoded credentials without requiring a real vault service.

    vault {
      # Active vault provider
      # Options: "local" (development), "aws", "azure", "gcp" (production)
      #
      # Local: Returns default credentials from local-vault-mapping.yaml
      # AWS: Invokes AWS Lambda function to fetch credentials
      # Azure: Calls Azure Function HTTP trigger
      # GCP: Calls GCP Cloud Function
      provider = "local"

      # Path to Rosetta mapping file (YAML or JSON)
      # Rosetta maps cloud-provider-specific vault responses to standard
      # VaultCredentials format (clientId, clientSecret, topic, role)
      #
      # Format: "classpath:path/to/mapping.yaml" (loads from JAR resources)
      #         or "file:/absolute/path/to/mapping.yaml" (loads from filesystem)
      #
      # The mapping file defines JSONPath expressions to extract credentials
      # from the vault response JSON and optional transformations (base64Decode,
      # toUpper, etc.)
      rosetta-mapping-path = "classpath:rosetta/local-vault-mapping.yaml"

      # Local vault configuration (used when provider = "local")
      local {
        # List of required fields to extract from vault response
        # topic and role are provided by TopicDirective, only credentials
        # need to be fetched from vault
        #
        # For local development, these are hardcoded in the mapping file
        required-fields = ["clientId", "clientSecret"]
      }
    }

    # -------------------------------------------------------------------------
    # Block Storage Configuration
    # -------------------------------------------------------------------------
    # Block storage provides access to test artifacts (feature files, topic
    # directives) and stores test evidence (Cucumber reports, logs) after
    # test execution.
    #
    # In production, Test-Probe downloads artifacts from cloud storage
    # (S3, Azure Blob, GCS) to JIMFS (in-memory filesystem) for execution,
    # then uploads evidence back to cloud storage.
    #
    # For local development, we use a "local" provider that uses JIMFS
    # without cloud storage integration.

    block-storage {
      # Active block storage provider
      # Options: "local" (development), "aws", "azure", "gcp" (production)
      #
      # Local: JIMFS-based in-memory filesystem (no cloud storage)
      # AWS: S3 bucket integration
      # Azure: Azure Blob Storage integration
      # GCP: Google Cloud Storage integration
      provider = "local"

      # Topic directive file name (YAML file containing Kafka topic configurations)
      # This file defines all topics, roles (producer/consumer), event types,
      # and serialization metadata (JSON, Avro, Protobuf)
      #
      # File location:
      #   - Local: Loaded from config/ directory
      #   - Production: Downloaded from cloud storage to JIMFS
      topic-directive-file-name = "topic-directive.yaml"
    }
  }
}

# =============================================================================
# Pekko Actor System Configuration
# =============================================================================
# Pekko (Apache Pekko) is a fork of Akka 2.6.x maintained by the Apache
# Software Foundation. It provides the actor runtime for Test-Probe.
#
# Note: This configuration is at the root level (not namespaced under
# test-probe) because Pekko requires top-level configuration.
#
# Key Concepts:
#   - Dispatcher: Thread pool for executing actors
#   - Parallelism: Number of threads in the pool
#   - Throughput: Messages processed per thread before switching actors

pekko {
  # Log level for Pekko actor system
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  # INFO: Logs actor lifecycle events (start, stop, restart)
  loglevel = "INFO"

  actor {
    # Default dispatcher for general-purpose actors
    # Used by: GuardianActor, QueueActor, TestExecutionActor
    default-dispatcher {
      # Fork-join executor: Work-stealing thread pool optimized for CPU-bound tasks
      fork-join-executor {
        # Minimum number of threads (scaled up based on CPU cores)
        parallelism-min = 2

        # Maximum number of threads
        # Recommendation: 8-16 for local development, 32-64 for production
        # Each thread can handle multiple actors (lightweight, non-blocking)
        parallelism-max = 8
      }
    }
  }
}
